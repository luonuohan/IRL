{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zjhY1UZvwVfl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aXXs73pLywfg"
      },
      "outputs": [],
      "source": [
        "class ParametricReward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 4 reward parameters\n",
        "        self.rho = nn.Parameter(torch.tensor(0.5))    # benchmark tracking\n",
        "        self.eta = nn.Parameter(torch.tensor(1.0))    # internal growth rate\n",
        "        self.lamb = nn.Parameter(torch.tensor(0.1))   # cashflow mismatch penalty\n",
        "        self.omega = nn.Parameter(torch.tensor(0.1))  # trade cost penalty\n",
        "\n",
        "    def forward(self, traj_batch):\n",
        "        total_rewards = []\n",
        "\n",
        "        for traj in traj_batch:\n",
        "            reward = 0.0\n",
        "            cash = traj['cash_t'][0]  # initial cash\n",
        "            for t in range(len(traj['x_t'])):\n",
        "                x_t = traj['x_t'][t]      # portfolio weights ($)\n",
        "                u_t = traj['u_t'][t]      # trades\n",
        "                r_t = traj['r_t'][t]      # realized sector returns\n",
        "                B_t = traj['B_t'][t]      # benchmark value\n",
        "                C_t = traj['C_t'][t]      # cash inflow (external)\n",
        "\n",
        "                trade_amount = torch.sum(u_t)\n",
        "                cash = cash + C_t - trade_amount  # cash_t+1 = cash_t + inflow - amount traded\n",
        "\n",
        "                # Value of new portfolio (after trade)\n",
        "                V_t = torch.dot(1 + r_t, x_t + u_t)\n",
        "\n",
        "                # Target portfolio value (PM’s goal)\n",
        "                P_hat_t = self.rho * B_t + (1 - self.rho) * self.eta * torch.sum(x_t)\n",
        "\n",
        "                # Reward function with penalty terms\n",
        "                reward += - (P_hat_t - V_t)**2 \\\n",
        "                          - self.lamb * (trade_amount - C_t)**2 \\\n",
        "                          - self.omega * torch.sum(u_t ** 2)\n",
        "\n",
        "            total_rewards.append(reward)\n",
        "\n",
        "        return torch.stack(total_rewards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bO6CkeX_zEwP"
      },
      "outputs": [],
      "source": [
        "# Loss: Softmax ranking (T-REX)\n",
        "def trex_loss(rewards, pair_indices):\n",
        "    # pair_indices: list of tuples (i, j) where traj[i] ≺ traj[j]\n",
        "    loss = 0.0\n",
        "    for i, j in pair_indices:\n",
        "        exp_i = torch.exp(rewards[i])\n",
        "        exp_j = torch.exp(rewards[j])\n",
        "        loss += -torch.log(exp_j / (exp_i + exp_j))\n",
        "    return loss / len(pair_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m8CK4aHhzXah"
      },
      "outputs": [],
      "source": [
        "def train_trex_model(traj_batch, pair_indices, lr=0.01, epochs=1000):\n",
        "    model = ParametricReward()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        rewards = model(traj_batch)\n",
        "        loss = trex_loss(rewards, pair_indices)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, \"\n",
        "                  f\"ρ={model.rho.item():.3f}, η={model.eta.item():.3f}, \"\n",
        "                  f\"λ={model.lamb.item():.3f}, ω={model.omega.item():.3f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-IXHdXYDtgq"
      },
      "outputs": [],
      "source": [
        "#This is an example what traj looks like\n",
        "traj = {\n",
        "    'x_t': List[Tensor],     # [T, 11]\n",
        "    'u_t': List[Tensor],     # [T, 11]\n",
        "    'r_t': List[Tensor],     # [T, 11]\n",
        "    'B_t': List[float],      # [T]\n",
        "    'C_t': List[float],      # [T]\n",
        "    'cash_t': List[float],   # [T+1]\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
