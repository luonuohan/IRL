{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dxZ8rnjFHAKc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATV4QKccGMkq",
    "outputId": "a78e7cfd-54b0-452b-86d4-27444c496f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Company Name      CIK  \\\n",
      "3831               SHELTON CAPITAL MANAGEMENT  1002784   \n",
      "5085   SHINE INVESTMENT ADVISORY SERVICES INC  1002912   \n",
      "17851                     BlackRock Group LTD  1003283   \n",
      "17852                     BlackRock Group LTD  1003283   \n",
      "17853                     BlackRock Group LTD  1003283   \n",
      "\n",
      "                     nameOfIssuer      cusip    value    shares shareType  \\\n",
      "3831   RENAISSANCERE HOLDINGS LTD  G7496G103    427.0    3070.0        SH   \n",
      "5085   RENAISSANCERE HOLDINGS LTD  G7496G103     27.0     193.0        SH   \n",
      "17851  RENAISSANCERE HOLDINGS LTD  G7496G103   1992.0   14627.0        SH   \n",
      "17852  RENAISSANCERE HOLDINGS LTD  G7496G103  22452.0  164825.0        SH   \n",
      "17853  RENAISSANCERE HOLDINGS LTD  G7496G103   1109.0    8142.0        SH   \n",
      "\n",
      "      putCall investmentDiscretion  votingAuthoritySole  \\\n",
      "3831      NaN                 SOLE               3070.0   \n",
      "5085      NaN                 SOLE                  0.0   \n",
      "17851     NaN                 SOLE              14627.0   \n",
      "17852     NaN                 SOLE              39387.0   \n",
      "17853     NaN                 SOLE               6270.0   \n",
      "\n",
      "       votingAuthorityShared  votingAuthorityNone filing_date  \\\n",
      "3831                       0                  0.0  2017-08-14   \n",
      "5085                       0                193.0  2017-08-01   \n",
      "17851                      0                  0.0  2017-02-10   \n",
      "17852                      0             125438.0  2017-02-10   \n",
      "17853                      0               1872.0  2017-02-10   \n",
      "\n",
      "                                              filing_url  \n",
      "3831   https://www.sec.gov/Archives/edgar/data/100278...  \n",
      "5085   https://www.sec.gov/Archives/edgar/data/100291...  \n",
      "17851  https://www.sec.gov/Archives/edgar/data/100328...  \n",
      "17852  https://www.sec.gov/Archives/edgar/data/100328...  \n",
      "17853  https://www.sec.gov/Archives/edgar/data/100328...  \n"
     ]
    }
   ],
   "source": [
    "# Load and filter the 13F dataset\n",
    "df = pd.read_csv(\"2017-01-01 to 2017-12-31 13f data.csv\")\n",
    "target_df = df[df[\"nameOfIssuer\"] == \"RENAISSANCERE HOLDINGS LTD\"]\n",
    "print(target_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXjPL6R1GuEW",
    "outputId": "eb585652-5bbb-45f1-f300-bd2864003c41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-4055238010.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  traj['cash_t'].append(torch.tensor(cash))\n"
     ]
    }
   ],
   "source": [
    "# Convert a row into a synthetic trajectory\n",
    "def convert_row_to_traj(row):\n",
    "    \"\"\"\n",
    "    Convert one row of 13F data into a dummy trajectory for MDP simulation.\n",
    "    Each step includes portfolio state, action, return, benchmark, and cash flow.\n",
    "    \"\"\"\n",
    "    shares = row[\"shares\"]\n",
    "    value = row[\"value\"]\n",
    "    dim = 11 # dimension of state/action vector\n",
    "\n",
    "    traj = {\n",
    "        'x_t': [],\n",
    "        'u_t': [],\n",
    "        'r_t': [],\n",
    "        'B_t': [],\n",
    "        'C_t': [],\n",
    "        'cash_t': []\n",
    "    }\n",
    "\n",
    "    x = torch.rand(dim) * 0.1 # initial portfolio weights\n",
    "    cash = 1.0  # initial cash reserve\n",
    "\n",
    "    for _ in range(6):  # simulate 6 time steps\n",
    "        u = torch.randn(dim) * 0.01 # portfolio adjustment\n",
    "        r = torch.randn(dim) * 0.02  # random return vector\n",
    "        B = torch.tensor(value * 0.001, dtype=torch.float32)  # benchmark proxy\n",
    "        C = torch.tensor(shares * 0.00001, dtype=torch.float32)  # cash flow proxy\n",
    "        cash = cash + C - torch.norm(u)\n",
    "\n",
    "        traj['x_t'].append(x)\n",
    "        traj['u_t'].append(u)\n",
    "        traj['r_t'].append(r)\n",
    "        traj['B_t'].append(B)\n",
    "        traj['C_t'].append(C)\n",
    "        traj['cash_t'].append(torch.tensor(cash))\n",
    "\n",
    "        x = x + u + torch.randn(dim) * 0.001  # simulate next state\n",
    "\n",
    "    return traj\n",
    "\n",
    "sample_traj = convert_row_to_traj(target_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4Ax3aOeXGyav"
   },
   "outputs": [],
   "source": [
    "# Define the hybrid reward network\n",
    "class HybridRewardNet(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.rho = nn.Parameter(torch.tensor(0.5))\n",
    "        self.eta = nn.Parameter(torch.tensor(1.0))\n",
    "        self.lamb = nn.Parameter(torch.tensor(0.1))\n",
    "        self.omega = nn.Parameter(torch.tensor(0.1))\n",
    "        self.nn_reward = nn.Sequential(\n",
    "            nn.Linear(dim * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, traj):\n",
    "        total_reward = 0.0\n",
    "        for t in range(len(traj['x_t'])):\n",
    "            x = traj['x_t'][t]\n",
    "            u = traj['u_t'][t]\n",
    "            r = traj['r_t'][t]\n",
    "            B = traj['B_t'][t]\n",
    "            C = traj['C_t'][t]\n",
    "            cash = traj['cash_t'][t]\n",
    "\n",
    "            V_t = torch.dot(1 + r, x + u) # estimated portfolio return\n",
    "            P_hat = self.rho * B + (1 - self.rho) * self.eta * torch.sum(x) # benchmark-adjusted target\n",
    "            penalty_cash = (torch.sum(u) - C) ** 2  # cash mismatch penalty\n",
    "            penalty_trade = torch.norm(u) ** 2  # transaction cost\n",
    "\n",
    "            reward_t = - (P_hat - V_t) ** 2 - self.lamb * penalty_cash - self.omega * penalty_trade\n",
    "            correction = self.nn_reward(torch.cat([x, u]))   # neural network correction\n",
    "            total_reward += reward_t + correction\n",
    "\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riwTgJXyG2ae",
    "outputId": "2c35b74d-033e-4bd0-ada2-cf204400ef3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Reward for RENAISSANCERE HOLDINGS LTD: -1.0359386205673218\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trajectory using the reward model\n",
    "model = HybridRewardNet(dim=11)\n",
    "reward_score = model(sample_traj)\n",
    "\n",
    "print(\"Estimated Reward for RENAISSANCERE HOLDINGS LTD:\", reward_score.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d241c85",
   "metadata": {},
   "source": [
    "# T-REX with HybridRewardNet and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956145fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRewardNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size=64, dropout=0.0):\n",
    "        super(HybridRewardNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc01bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_samples = 500\n",
    "state_dim = 10\n",
    "\n",
    "states = torch.tensor(np.random.rand(num_samples, state_dim), dtype=torch.float32)\n",
    "labels = torch.tensor(np.random.randint(0, 2, size=(num_samples, 1)), dtype=torch.float32)\n",
    "\n",
    "# Split into train and validation\n",
    "states_train, states_val, labels_train, labels_val = train_test_split(states, labels, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9503b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_loss(model, states, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(states)\n",
    "        loss = nn.BCEWithLogitsLoss()(pred, labels)\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f085bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_size': [32, 64],\n",
    "    'learning_rate': [1e-3, 1e-4],\n",
    "    'dropout': [0.0, 0.2],\n",
    "    'epochs': [5, 10]\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for hs, lr, dr, ep in product(param_grid['hidden_size'],\n",
    "                              param_grid['learning_rate'],\n",
    "                              param_grid['dropout'],\n",
    "                              param_grid['epochs']):\n",
    "\n",
    "    model = HybridRewardNet(input_dim=state_dim, hidden_size=hs, dropout=dr)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(ep):\n",
    "        model.train()\n",
    "        for s, label in zip(states_train, labels_train):\n",
    "            pred = model(s.unsqueeze(0))\n",
    "            loss = loss_fn(pred, label.unsqueeze(0))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    val_loss = compute_validation_loss(model, states_val, labels_val)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = model\n",
    "        best_params = (hs, lr, dr, ep)\n",
    "\n",
    "print(\"Best params:\", best_params)\n",
    "print(\"Best validation loss:\", best_loss)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
